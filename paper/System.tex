\section{System}
\label{sec:System}

For proving store-atomicity of the memory subsystem, it is sufficient to model
just the memory subsystem, instead of modeling the processors.
The memory subsystem consists of a tree hierarchy of caches. These
caches are inclusive, \ie a cache at a higher level (near the root) contains
all the addresses present in caches in the lower levels of its substree. The
root of the communicates with the main memory, and the leaf caches communicate
with the processor (Figure \ref{hier}). Whether a cache is a leaf or not is
denoted by the relation \leaf$(c)$.

We will restrict the discussion of the formal proofs to the MSI protocol
\cite{MSI}. In the MSI protocol, each cache maintains a permission for each
address, denoting whether that address can be read and modified (\Mo), only
read (\Sh) or neither read nor modified (\In) in that cache. Consequently, in a
leaf cache, a load and store request can be processed by a cache only if the
cache is at least in \Sh{} and \Mo{} states, respectively, for the corresponding
address. For each address, caches which are in the subtree of a cache can not
have higher permissions than the latter. Extension of this proof framework to
other protocols like MESI and MOESI is straightforward and will be discussed
informally.

Each leaf cache $c$ has a \emph{position} denoted by $c.\pos$, which represents
the position of the next request from the processor corresponding to $c$ that
it has to process. Since it has to process the first request first, it is set
to $0$ initially.

Each cache $c$, in addition to the data associated with each address $a$ (denoted
by $c.\data[a]$), also has a \emph{coherence state} associated with the
address (denoted by $c.\state[a]$). The coherence state can be
one of the three values: \Mo{} (Modified), \Sh{} (Shared) or \In{} (Invalid).
\Mo{} state denotes that the subtree rooted at the cache in \Mo{} state for
that address can modify the data, \ie{} both read and write the data
corresponding to the address. \Sh{} state denotes that the subtree rooted at
the cache in \Sh{} state for that address can only read the data corresponding
to that address and \In{} state denotes that the subtree rooted at the cache in
\In{} state can neither read nor write the data corresponding to the address.
The meanings of the state is clear, but one must verify the actual cache
coherence protocol for violations of these invariants, which is what the proof
in this paper intends to do.  There's a logical ordering between the states, in
terms of the permissions that they have: $\Mo > \Sh > \In$ (the notation should
be read assuming that the order is transitively closed). The states for each of
the addresses in each of the caches, other than the root cache, is set to \In,
while states for each address in the root cache is set to \Mo. This is because
only the main memory (connected to the root cache) has the initial copy of
data.  In addition, each non-leaf cache maintains a \emph{directory}, which
contains a local version of the state of each of its children caches for a
particular address.  For a non-leaf cache $p$, the state in its directory for
child cache $c$ is denoted by $p.\dstate[c][a]$, for address $a$.  The relation
$\parent(c, p)$ denotes that $p$ is the parent of cache $c$.  This local
version may be stale, but it should not be lower than the real state of the
child (again this property has to be proven for a cache coherence protocol).
All the directory states are set to \In initially because initially, all the
non-root caches have their states for all the addresses set to \In.

In order for a cache to change to a higher state for a given address, \ie
\emph{upgrade}, it has to \emph{request} its parent cache for increasing the
state.  For a cache to change to a lower state, \ie \emph{downgrade}, it has to
notify its parent cache indicating the downgrade -- it uses \emph{response}
messages for this purpose. On the other hand, for a parent cache to force a
child cache to downgrade the child's state for an address (in order to give
permissions to a different child cache), or equivalently, to downgrade the
parent's directory state for the child, it has to \emph{request} that child
cache to downgrade, and a parent can notify a child about upgrading the child's
state, or equivalently, about upgrading the parent's directory state for the
child, by sending a \emph{response} message.

Each cache is connected to its parent cache using 3 logical FIFO connections: a)
\cpReq{} channel to transmit requests from the child to the parent, b) \cpResp{}
channel to transmit responses from the child to the parent, and c) \pc{}
channel to transmit both request and response messages from
parent to child.

%In addition, the leaf caches (called L1 caches usually)
%communicate with the processor through two channels: a) \cproc{} \textbf{FIFO}
%channel to transmit requests from the processor to L1 cache, and b) \procc{}
%\textbf{FIFO} channel to transmit responses from the L1 cache to the processor.
%There is exactly one of \cproc{} and \procc{} channels between the L1 and the processor.

In this paper, we will assume that each channel has a dedicated link between
the communicating caches. In a real system, these channels will be logically
implemented over an interconnect network, which poses additional problems that
have to be proven -- namely that of deadlock and starvation-freedom. Though we
have proved these results for a general interconnect network as opposed to a
point-to-point connection between caches, we completely omit the formal
discussion of deadlock and starvation freedom in this paper because of lack of
space. We will informally discuss how deadlock and starvation freedom
properties are proved for our system.

Since we have dedicated channel between each pair of child-parent, we will
index the three channels (\cpReq{}, \cpResp{} and \pc{}) by the name of the
child that uses that channel (using the notation $\cpReq[c]$, \etc for the
\cpReq{} channel associated with cache $c$).  Figure \ref{format} shows the
message format transmitted in the 3 channels between caches. In each of the
channels $ch$, a message $m$ can be enqueued ($\enq(ch, m)$) or dequeued
($\deq(ch)$), and the channels can be examined for the first element
($\first(ch)$) or for availability of any message in the channel ($\avail(ch)$).
Each of these channels start out empty \ie $\forall ch, \neg \avail(ch)$, initially.
%The messages transmitted between the processor and the L1 cache
%is the same as in Figure \ref{req-resp} (except for the omission of the \proc{}
%field, which is indicated by the channel's index).
\begin{figure}
\begin{subfigure}{6.8cm}
\begin{tabular}{|lp{5.8cm}|}
\hline
\multicolumn{2}{|c|}{\Reqcp}\\
\hline
\from: & Current state of child\\
\myto: & Desired (higher) state of child\\
\addr: & Address associated with the request\\
\hline
\hline
\multicolumn{2}{|c|}{\Respcp}\\
\hline
\from: & Current state of child\\
\myto: & Downgraded state of child\\
\addr: & Address associated with the request\\
\data: & Data associated with the address, if necessary\\
\hline
\end{tabular}
\end{subfigure}
\begin{subfigure}{5.4cm}
\begin{tabular}{|lp{4.4cm}|}
\hline
\multicolumn{2}{|c|}{\Mpc}\\
\hline
\typ: & \Req{} (for requests) and \Resp{} (for responses)\\
\myto: & Desired (lower) directory state of a child for \Req{} and upgraded
directory state of a child for \Resp{}\\
\addr: & Address associated with the request\\
\data: & Data associated with the address if necessary (for \Resp{} only)\\
\hline
\end{tabular}
\end{subfigure}
\caption{Data types for messages between caches}
\label{format}
\end{figure}

In addition to the actual coherence states, each cache also maintains a
temporary \emph{wait state} for each of its addresses. The wait state contains
two parts: a) a bool representing whether the cache has sent a request to its
parent cache earlier and is waiting for a response (denoted by $c.\wait[a]$ for
address $a$) and b) one of the three values \Mo, \Sh{} or \In representing the
state upgrade that the cache is waiting for (denoted by $c.\waitS[a]$ for
address $a$). Similarly, each non-leaf cache has the corresponding wait states
for its directory.  $p.\dwait[c][a]$ denotes that cache $p$ is waiting for a
response from its child $c$ for address $a$, and $p.\dwaitS[c][a]$ denotes the
state that cache $p$ is waiting for $c$ to downgrade to. Both \wait{} and
\dwait{} are \False{} initially for each of the caches, and each of the
addresses, because no cache is waiting for any response from either its child
or its parent initially.

The behavior of the system can be given in terms of atomic state transitions.
These transitions have two parts: a) the guard, which dictates when the
transition can fire, and b) the action, which dictates how the state of the
system changes when the transition happens. These transitions or \emph{guarded
atomic actions} logically happen one by one, making the overall behavior of the
system well defined. Figure \ref{trans} gives the atomic transitions for a
cache coherent memory subsystem. The actions in the transitions repesent only
the specific system states that change. We use $x \Leftarrow y$ to indicate
setting of a specific state $x$ of the overall system to value $y$ because of a
transition.

Figure \ref{childside} shows the transitions that result in an upgrade of the
state of a cache. It first involves a cache sending an upgrade request to the
its parent when the cache is not already waiting for a response for that
address from the parent (ChildSendReq). This request is received by the parent
(ParentRecvReq), and if the rest of the parent's children caches are
\emph{compatible}, and the parents state also has the permission being
requested, it sends an upgrade response to the cache, while changing the
appropriate directory state. Compatibility of an upgrade request with respect
to a cache's siblings can be defined as follows:
\begin{multline*}
\compat = \lambda (p \in Cache) (c \in Cache) (a \in \textit{Addr}) (s \in \{\Mo, \Sh, \In\}) \Rightarrow \forall i, i \ne c @-> \\
\mylet s_i := p.\dstate[i][a] \myin\\
 (s = \Mo @-> s_i = \In) \wedge (s = \Sh @-> s_i = \Sh) \wedge (s = \In @-> s_i = \Mo)
\end{multline*}
Finally, the child cache receives the response from the parent (ChildRecvResp)
and sets its state while reseting its wait state if the desired upgrade is
indicated by the response.

Figure \ref{parentside} shows the transitions that result in a downgrade of the
state of a cache because of a request by the cache's parent when the parent is
not already waiting for a response from that cache (ParentSendReq).  This
request is received by the child (ChildRecvReq), and if the state of each of
the child's own children is not greater than the requested downgrade, the cache
downgrades its state and sends a notification response to the parent. Finally,
the parent receives the response from the cache (ParentRecvResp) and sets the
appropriate directory state while reseting the appropriate directory wait state
if the desired downgrade is indicated by the response.

Figure \ref{childextra} shows transitions that happen in a non-root cache
because of the presence of \emph{voluntary} responses. Whenever a cache runs
out of space to store the data corresponding to an address, it evicts the data
corresponding to one or more addresses to make room for the new address. This
eviction causes the state of the cache for that address to go to $\In$, which
has to be informed to its parent. We use the same \cpResp{} FIFO (and the same
response format) to indicate this. Such a voluntary response for an address can
take place only if the cache is not waiting for a response from its parent for
that address (ChildVolResp).

Because of the presence of voluntary response, a child cache could have already
downgraded to a state required by a downgrade request from its parent. In such
a case, the cache simply drops the downgrade request (ChildDropReq).

Figure \ref{procside} shows the transitions that actually serve a processors
request in the leaf caches. LoadReq and StoreReq show the processing of a load
and store request, respectively, from a processor. In the case of a store
request, the data in the cache for the requested address is updated with that
supplied in the request. We do not explicitly show the response generated by
the cache in case of a load request in our memory subsystem model; in a real
system, the data in the cache for the requested address is sent back to the
processor. Our definition of store atomicity requires responses for both load
and store requests, and the response field contains the
$(\textit{Processor}*\mathbb{N})$ pair identifying the request corresponding to
the response, the data from the cache in case of a load response, and a value
for the field \tme. A \Response{} is defined whenever LoadReq or StoreReq
transition takes place. Let $c$ be a leaf cache, and $t$ be the position of a
LoadReq$(c)$ or StoreReq$(c)$ transition in the ordered list of transitions of
the full system starting from the initial state. The fields of the response are
as follows:
\begin{multline*}
\small\langle (c, c.\pos), t, \mylet q := \reqFn(c, c.\pos) \myin\\
\small\myif q.\desc = \Ld \mythen c.\data[q.\addr] \myelse \_ \rangle
\end{multline*}


\begin{figure}
\small
\centering
\begin{subfigure}{\textwidth}
\centering
\begin{tabular}{|ll|}
\hline
\multicolumn{2}{|l|}{\textbf{ChildSendReq}$(c, x, a)$: Child $c$ sending request to upgrade to $x$ for address $a$}\\
\hline
Guard: & $c.\state[a] < x \wedge c.\wait[a] = \False$\\
\hline
Action: & $c.\waitS[a] \Leftarrow x$; $c.wait[a] \Leftarrow \True$; $\enq(\cpReq[c], \langle c.\state[a], x, a \rangle)$;\\
\hline
\hline
\multicolumn{2}{|l|}{\textbf{ParentRecvReq}$(p, c)$: Parent $p$ receiving a request from child $c$}\\
\hline
Guard: & 
$\parent(c,p) \wedge \avail(\cpReq[c]) \wedge \mylet q := \first(\cpReq[c]) \myin$\\
& $p.\dwait[c][q.\addr] = \False \wedge p.\dstate[c][q.\addr] \le q.\from \wedge$\\
& $\compat(p, c, q.\addr, q.\myto) \wedge q.\myto \le p.\state[q.\addr]$\\
\hline
Action: & $\enq(\pc[c], \langle \Resp, q.\myto, q.\addr, $\\
& $\;\;\;%
\myif p.\dstate[c][q.\addr] = \In \mythen p.\data[q.\addr] \myelse \_\rangle)$;\\
& $p.\dstate[c][q.\addr] \Leftarrow q.\myto$; $\deq(\cpReq[c])$;\\
\hline
\hline
\multicolumn{2}{|l|}{\textbf{ChildRecvResp}$(c)$: Child $c$ receiving a response}\\
\hline
Guard: & 
$\avail(\pc[c]) \wedge \mylet r := \first(\pc[c]) \myin r.\typ = \Resp$\\
\hline
Action: & $\myif c.\state[r.\addr] = \In \mythen c.\data[a] \Leftarrow r.\data$;\\
&$\myif c.\waitS[r.\addr] \le r.\myto \mythen c.\wait[a] \Leftarrow \False $;\\
& $c.\state[r.\addr] \Leftarrow r.\myto$; $\deq(\pc[c])$;\\
\hline
\end{tabular}
\caption{Child sending upgrade request and getting back the response}
\label{childside}
\end{subfigure}

\begin{subfigure}{\textwidth}
\centering
\begin{tabular}{|ll|}
\hline
\multicolumn{2}{|l|}{\textbf{ParentSendReq}$(p, c, x, a)$: Parent $p$ sending request to child $c$ to downgrade to $x$ for address $a$}\\
\hline
Guard: & $\parent(c,p) \wedge p.\dstate[c][a] > x \wedge p.\dwait[c][a] = \False$\\
\hline
Action: & $p.\dwaitS[c][a] \Leftarrow x$; $p.\dwait[c][a] \Leftarrow \True$; $\enq(\pc[c], \langle \Req, x, a, \_ \rangle)$;\\
\hline
\hline
\multicolumn{2}{|p{\textwidth}|}{\textbf{ChildRecvReq}$(c)$: Child $c$ receiving a request from its parent and has to downgrade}\\
\hline
Guard: & 
$\avail(\pc[c]) \wedge \mylet q := \first(\pc[c]) \myin q.\typ = \Req \wedge$ \\
& $(\forall i, \parent(i, c) \rightarrow c.\dstate[i][q.\addr] \le q.\myto) \wedge q.\myto < c.\state[q.\addr]$\\
\hline
Action: & $\enq(\cpResp[c], \langle c.\state[q.\addr], q.\myto, q.\addr,$\\
& $\;\;\;%
\myif c.\state[q.\addr] = \Mo \mythen c.\data[q.\addr] \myelse \_\rangle)$;\\
& $c.\state[q.\addr] \Leftarrow q.\myto$; $\deq(\pc[c])$;\\
\hline
\hline
\multicolumn{2}{|l|}{\textbf{ParentRecvResp}$(p, c)$: Parent $p$ receiving a response from child $c$}\\
\hline
Guard: & 
$\parent(c,p) \wedge \avail(\cpResp[c]) \wedge \mylet r := \first(\cpResp[c]) \myin$\\
& $r.\from = p.\state[c][r.\addr]$\\
\hline
Action: & $\myif p.\dstate[c][r.\addr] = \Mo \mythen p.\data[r.\addr] \Leftarrow r.\data$;\\
&$\myif p.\dwaitS[c][r.\addr] \ge r.\myto \mythen p.\dwait[c][r.\addr] \Leftarrow \False $;\\
& $p.\dstate[c][r.\addr] \Leftarrow r.\myto$; $\deq(\cpResp[c])$;\\
\hline
\end{tabular}
\caption{Parent sending downgrade request and getting back the response}
\label{parentside}
\end{subfigure}

\begin{subfigure}{\textwidth}
\centering
\begin{tabular}{|ll|}
\hline
\multicolumn{2}{|p{\textwidth}|}{\textbf{ChildVolResp}$(c, x, a)$: Child $c$ sending a response to downgrade to $x$ for address $a$ without any request from its parent}\\
\hline
Guard: & $(\forall i, \parent(i, c) \rightarrow c.\dstate[i][a] \le x) \wedge x < c.\state[a] \wedge c.\wait[a] = \False$\\
\hline
Action: & $\enq(\cpResp[c], \langle c.\state[a], x, a,%$\\
%& $\;\;\;%
\myif c.\state[a] = \Mo \mythen c.\data[a] \myelse \_\rangle)$;\\
& $c.\state[a] \Leftarrow x$;\\
\hline
\hline
\multicolumn{2}{|p{\textwidth}|}{\textbf{ChildRecvBadReq}$(c)$: Child $c$ receiving a request from its parent and has already downgraded}\\
\hline
Guard: & 
$\avail(\pc[c]) \wedge \mylet q := \first(\pc[c]) \myin q.\typ = \Req \wedge%$\\
%& $%
q.\myto \ge c.\state[q.\addr]$\\
\hline
Action: & $\deq(\pc[c])$;\\
\hline
\end{tabular}
\caption{Other transitions at a non-root cache}
\label{childextra}
\end{subfigure}

\begin{subfigure}{\textwidth}
\centering
\begin{tabular}{|ll|}
\hline
\multicolumn{2}{|l|}{\textbf{LoadReq}$(c)$: Handling a processor's load request at a leaf cache}\\
\hline
Guard: & $\leaf(c) \wedge \mylet q := \reqFn(c, c.\pos) \myin q.\desc = \Ld \wedge c.\state[q.\addrQ] \ge \Sh$\\
\hline
Action:& $c.\pos \Leftarrow c.\pos + 1$;\\
\hline
\hline
\multicolumn{2}{|l|}{\textbf{StoreReq}$(c)$: Handling a processor's store request at a leaf cache}\\
\hline
Guard: & $\leaf(c) \wedge \mylet q := \reqFn(c, c.\pos) \myin q.\desc = \St \wedge c.\state[q.\addrQ] = \Mo$\\
\hline
\hline
Action:& $c.\pos \Leftarrow c.\pos + 1$; $c.\data[q.\addrQ] \Leftarrow q.\dataQ$;\\
\hline
\end{tabular}
\caption{Handling requests from the processor}
\label{procside}
\end{subfigure}
\caption{Atomic transitions for a cache coherent memory subsystem}
\label{trans}
\end{figure}

Note that the atomic transitions in Figure \ref{trans} access only local states,
\ie a transition can read or write states only corresponding to a single cache
and/or the channel the cache is connected to. This restriction allows such a
system to be implemented directly into hardware. Bluespec System Verilog (BSV),
for instance, directly converts these transitions into efficient synchronous
hardware. Though these transitions logically happen one by one, BSV executes
several transitions simultaneously, as long as they do not access the same
state, \ie do not \emph{conflict} \cite{Hoe:TCAD,HoeArvind:TRSSynthesis1}.
Sometimes, these transitions (both the guards and the actions) may not be
amenable to single cycle implementations in hardware. For example, writing a
register array (like a cache) can actually take several hardware clock cycles,
though logically it is a single transition.  Karczmarek \etal \cite{Karczmarek}
has provided a scheme to convert atomic transitions into synchronous hardware
in which each transition can span several clock cycles.

