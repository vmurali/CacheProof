\section{System}
\label{sec:System}

For proving store-atomicity of the memory subsystem, it is sufficient to model
just the memory subsystem, instead of modeling the processors.
The memory subsystem consists of a tree hierarchy of caches. These
caches are inclusive, \ie a cache at a higher level (near the root) contains
all the addresses present in caches in the lower levels of its substree. The
root of the communicates with the main memory, and the leaf caches communicate
with the processor (Figure \ref{hier}).

Each cache has a \emph{coherence state} associated with each address. The
coherence state can be one of the three values: \Mo{} (Modified), \Sh{} (Shared) or
\In{} (Invalid). \Mo{} state denotes that the subtree rooted at the cache in \Mo{}
state for that address can modify the data, \ie{} both read and write the data
corresponding to the address. \Sh{} state denotes that the subtree rooted at the
cache in \Sh{} state for that address can only read the data corresponding to
that address and \In{} state denotes that the subtree rooted at the cache in \In{}
state can neither read nor write the data corresponding to the address. The
meanings of the state is clear, but one must verify the actual cache coherence
protocol for violations of these invariants, which is what the proof in this
paper intends to do. There's a logical ordering between the states, in terms of
the permissions that they have: $\Mo > \Sh > \In$ (the notation should be read
assuming that the order is transitively closed).
In addition, each non-leaf cache maintains a \emph{directory}, which contains a
local version of the state of each of its children caches for a particular address.
This local version may be stale, but it should not be lower than the real state
of the child (again this property has to be proven for a cache coherence
protocol).

Though this paper restricts the formal discussion to the simple MSI protocol,
extension of this proof technique as well as major portions of the proof to
other protocols like MOSI and MOESI is straightforward.

In order for a cache to change to a higher state for a given address, \ie
\emph{upgrade}, it has to \emph{request} its parent cache for increasing the
state.  For a cache to change to a lower state, \ie \emph{downgrade}, it has to
notify its parent cache indicating the downgrade -- it uses \emph{response}
messages for this purpose. On the other hand, for a parent cache to force a
child cache to downgrade the child's state for an address (in order to give
permissions to a different child cache), or equivalently, to downgrade the
parent's directory state for the child, it has to \emph{request} that child
cache to downgrade, and a parent can notify a child about upgrading the child's
state, or equivalently, about upgrading the parent's directory state for the
child, by sending a \emph{response} message.

Each cache is connected to its parent cache using 3 logical FIFO connections: a)
\cpReq{} channel to transmit requests from the child to the parent, b) \cpResp{}
channel to transmit responses from the child to the parent, and c) \pc{}
channel to transmit both request and response messages from
parent to child.

%In addition, the leaf caches (called L1 caches usually)
%communicate with the processor through two channels: a) \cproc{} \textbf{FIFO}
%channel to transmit requests from the processor to L1 cache, and b) \procc{}
%\textbf{FIFO} channel to transmit responses from the L1 cache to the processor.
%There is exactly one of \cproc{} and \procc{} channels between the L1 and the processor.

In this paper, we will assume that each channel has a dedicated link between
caches. In a real system, these channels will be logically implemented over an
interconnect network, which poses additional problems that have to be proven --
namely that of deadlock and starvation-freedom. Though we have proved these
results for a general interconnect network as opposed to a point-to-point
connection between the communicating caches, we completely omit the formal
discussion of deadlock and starvation freedom in this paper because of lack of
space. We \xxx{might later} informally discuss how deadlock and starvation
freedom properties are proved for our system.

Since we have dedicated channel between each pair of child-parent, we will
index the three channels (\cpReq{}, \cpResp{} and \pc{}) by the name of the
child that uses that channel.
%We will also index the two channels (\cproc{} and
%\procc{}) between the processor and L1 cache by the name of the L1 cache.
Figure \ref{format} shows the message format transmitted in the 3 channels
between caches.
%The messages transmitted between the processor and the L1 cache
%is the same as in Figure \ref{req-resp} (except for the omission of the \proc{}
%field, which is indicated by the channel's index).
\begin{figure}
\begin{subfigure}{6.8cm}
\begin{tabular}{|lp{5.8cm}|}
\hline
\multicolumn{2}{|c|}{\Reqcp}\\
\hline
\from: & Current state of child\\
\myto: & Desired (higher) state of child\\
\addr: & Address associated with the request\\
\hline
\hline
\multicolumn{2}{|c|}{\Respcp}\\
\hline
\from: & Current state of child\\
\myto: & Downgraded state of child\\
\addr: & Address associated with the request\\
\data: & Data associated with the address, if necessary\\
\hline
\end{tabular}
\end{subfigure}
\begin{subfigure}{5.4cm}
\begin{tabular}{|lp{4.4cm}|}
\hline
\multicolumn{2}{|c|}{\Mpc}\\
\hline
\typ: & \Req{} (for requests) and \Resp{} (for responses)\\
\myto: & Desired (lower) directory state of a child for \Req{} and upgraded
directory state of a child for \Resp{}\\
\addr: & Address associated with the request\\
\data: & Data associated with the address if necessary (for \Resp{} only)\\
\hline
\end{tabular}
\end{subfigure}
\caption{Data types for messages between caches}
\label{format}
\end{figure}

The behavior of the system can be given in terms of atomic state transitions.
These transitions have two parts: a) the guard, which dictates when the
transition can fire, and b) the action, which dictates how the state of the
system changes when the transition happens. These transitions or \emph{guarded
atomic actions} logically happen one by one, making the overall behavior of the
system well defined.

The atomic transitions access local states, \ie a transition can read or write
states only corresponding to a single cache and/or the channel the cache is
connected to. This restriction allows such a system to be implemented directly
into hardware. Bluespec System Verilog (BSV), for instance, directly converts
these transitions into efficient synchronous hardware. Though these transitions
logically happen one by one, BSV executes several transitions simultaneously,
as long as they do not access the same state, \ie do not \emph{conflict}
\cite{Hoe:TCAD,HoeArvind:TRSSynthesis1}. Sometimes, these transitions (both the
guards and the actions) may not be amenable to single cycle implementations in
hardware. For example, writing a register array (like a cache) can actually
take several hardware clock cycles, though logically it is a single transition.
Karczmarek \etal \cite{Karczmarek} has provided a scheme to convert atomic
transitions into synchronous hardware in which each transition can span several
clock cycles.
