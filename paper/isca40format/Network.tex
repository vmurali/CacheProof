\section{Network, resource and scheduler requirements}
\label{sec:network}

In this section we will spell out all the requirements that are needed in terms
of ordering of messages between a pair of caches, blocking of messages in the
network, the minimum amount of thread resources needed, and the requirements
for scheduling a thread. This hides all the complexity of the distributed MSI
protocol, so that the protocol can be reasoned about one thread at a time, as
if the threads in each node were non-suspensive.

\subsection{Message ordering requirements}
\floatstyle{boxed}
\restylefloat{figure}

\begin{figure}\small
\begin{requirement}
An incoming response for an address $a$ from a child $c$ should not be
received before another incoming response from the same $c$ and the
same $a$ sent earlier has been received\label{cRespFifo}
\end{requirement}
\begin{requirement}
An incoming request for an address $a$ from a source $n$ should not be
received before another incoming response from the same $n$ and the
same $a$ sent earlier has been received\label{reqNoOvertakeResp}
\end{requirement}
\caption{Ordering requirements}
\label{order}
\end{figure}

In the distributed MSI protocol presented in Section
\ref{sec:DistributedMsi}, we were agnostic about the properties of the input and
output channels of the caches. Figure \ref{order} gives the exact ordering
requirements between messages transmitted between the caches.

We will illustrate the need for Requirements \ref{cRespFifo} and
\ref{reqNoOvertakeResp} using a 2-level of caches having two L1 caches $c_1$ and
$c_2$, and one shared L2 cache $p$. Each core can have several outstanding
memory requests.

\floatstyle{plain}
\restylefloat{figure}
\begin{figure}
\centering
\includegraphics[scale=0.4]{complicated}
\caption{Effect of violating Requirement \ref{cRespFifo} (order between responses)}
\label{complicated}
\end{figure}
\floatstyle{boxed}
\restylefloat{figure}

Let's say $c_1.state[a] = p.dir[c_1][a] = M$, $c_2.state[a] = p.dir[c_2][a] =
I$. $c_2$ gets a load request from its processor for address $a$ and sends an
upgrade request to $p$. $p$ sends a downgrade-to-$S$ request to $c_1$. $c_1$
receives the downgrade-to-$S$ request and downgrades to $S$, sending a downgrade
response to $p$. $c_1$ decides to evict address $a$ to make room for another
address $a'$, because of a request from the core for $a'$ not present in $c_1$.
$c_1$ sends a downgrade-to-$I$ response to $p$. Let's say the second downgrade
to $I$ response reaches $p$ first, violating Requirement \ref{cRespFifo}. $p$
sends an upgrade-to-$S$ response to $c_2$. $c_2$ receives the upgrade response
and changes state to $S$. $p$ then receives the second downgrade-to-$S$ response
from $c_1$, changing its $p.dir[c_1][a]$ to $S$. $c_2$ sends an upgrade-to-$M$
request to $p$ because of a store from its core for $a$. $p$ will send a
downgrade-to-$I$ request to $c_1$, but $c_1$ drops the request as seen in
procedure \dReq{} in Figure \ref{realistic}. So $p$ will never receive a
response from $c_1$ leading to a deadlock. This is illustrated in Figure
\ref{complicated}.

\floatstyle{plain}
\restylefloat{figure}
\begin{figure}
\centering
\includegraphics[scale=0.4]{lessComplicated}
\caption{Effect of violating Requirement \ref{reqNoOvertakeResp} (requests overtaking responses)}
\label{lessComplicated}
\end{figure}
\floatstyle{boxed}
\restylefloat{figure}

Consider another scenario starting with $c_1.state[a] = p.dir[c_1][a] = S$ and
$c_2.state[a] = p.dir[c_2][a] = I$. $c_1$ receives a store request and sends an
upgrade-to-$M$ request to $p$. $p$ sends back an upgrade-to-$M$ response to
$c_1$. $c_2$ gets a load request for address $a$ and sends an upgrade-to-$S$
request to $p$. $p$ sends a request to $S$ to $c_1$. Suppose Requirement
\ref{reqNoOvertakeResp} is violated and the request to $S$ reaches $c_1$ before
the response to $M$. $c_1$ will drop the request since its already in state $S$,
and so $p$ will never get a response leading to a deadlock. This is illustrated
in Figure \ref{lessComplicated}.

\subsection{Resource requirements and blocking restrictions}

\begin{figure}\small
\begin{requirement}
At a cache node, an incoming request from the parent should not be blocked by any
incoming request from its children.\label{cReqNoBlockPReq}
\end{requirement}
\begin{requirement}
At a cache node, an incoming response from any of its children should not
be blocked by any incoming requests.\label{reqNoBlockResp}
\end{requirement}
\begin{requirement}
At a cache node, an incoming response for address $a$ from its parent should not be blocked by
an incoming request from its child for address $a$. \label{reqBlockResp2}
\end{requirement}
\caption{Blocking restrictions}
\label{blocking}
\end{figure}

Figure \ref{blocking} shows the restrictions that the input channels should
obey with respect to an incoming message blocking other incoming messages.

\begin{figure}\small
\begin{requirement}
Each cache node has at least one dedicated thread resource for handling only messages
(requests or responses) from the parent\label{dedicate1}
\end{requirement}
\begin{requirement}
Each cache node has at least one dedicated thread resource for handling only responses
(from children or from the parent)\label{dedicate2}
\end{requirement}
\begin{requirement}
Each cache node has at least three thread resources\label{dedicate3}
\end{requirement}
\caption{Dedicated resources in a cache node}
\label{dedicated}
\end{figure}

Figure \ref{dedicated} shows the minimum number of thread resources that should
be present in each cache node. Lack of required thread resouces leads to the same
kindof deadlock scenarios as incorrect blocking does. This is illustrated in the
following:

Let's say both $c_1.state[a] = p.dir[c_1][a] = S$ and $c_2.state[a] =
p.dir[c_2][a] = S$. Both $c_1$ and $c_2$ get store requests from the processor
for address $a$, and they send upgrade-to-$M$ requests to $p$.  $p$ receives the
request from $c_2$ first. $p$ sends a downgrade-to-$I$ request to $c1$.

\floatstyle{plain}
\restylefloat{figure}
\begin{figure}
\centering
\includegraphics[scale=0.4]{unknown}
\caption{Effect of violating Requirement \ref{cReqNoBlockPReq} (requests blocking responses from parent)}
\label{unknown}
\end{figure}
\floatstyle{boxed}
\restylefloat{figure}

Let's say Requirement \ref{cReqNoBlockPReq} is not held, and instead some request
from the core is blocking $c_1$ from receiving the request. $c_1$ will never
respond back to $p$. But $p$ will be waiting in procedure \dReqL{}, and will
never start handling the request from $c_1$. This will create a deadlock.  The
same deadlock is also created when $c_1$ is out of thread resources to create a
thread to handle the downgrade request from $p$ (violating Requirement
\ref{dedicate1}). Figure \ref{unknown} depicts this scenario.

Let's extend this scenario further. Instead of violating Requirements
\ref{cReqNoBlockPReq} or \ref{dedicate1}, let $c_1$
send a downgrade-to-$I$ response to $p$. Let's instead assume that Requirement
\ref{reqNoBlockResp} is violated in $p$. So, the downgrade-to-$I$ response from
$c_1$ is not received by $p$, leading to the same deadlock. This is the same if
Requirement \ref{dedicate2} is violated and $p$ is out of thread resources to
create a thread to handle the downgrade response.

The need for Requirement \ref{dedicate3} can be shown easily. Let's say that a
cache has exactly 2 thread resources, one for handling messages from parents,
and one for handling responses from either the parent or the children. Now, if
it receives a request from one of its children, it can not use either of the
thread resources. Thus, at least one more thread resource is needed for
handling requests from children. This thread resource can also be used for
handling other types of incoming message.

\subsection{Scheduler requirements}

We will now list the requirements that a scheduler must obey to avoid
deadlocks as well as to give the illusion of non-suspensive threads.

\begin{figure}\small
\begin{requirement}
$p$ can not receive a request \Req{c}{p}{a}{x} from $c$, where $p = c.parent$,
if either
\begin{enumerate}
\item address $a$ has been chosen for eviction by some thread and that thread
has not yet replaced $a$, or
\item a thread in $p$ is handling a request for address
$a$ from any source
\end{enumerate}
\label{pHandleReq}
\end{requirement}
\begin{requirement}
$c$ can not receive a request \Req{p}{c}{a}{x} from $p$, where $p = c.parent$,
if either
\begin{enumerate}
\item address $a$ has been chosen for eviction by some thread and that thread
has not yet replaced $a$, or
\item a thread in $c$ is handling a request for address
$a$ from $p$, or
\item a thread in $c$ is handling a request for address
$a$ from one of $c$'s children $c'$, but that thread is not waiting for a
response from $p$
\end{enumerate}
\label{cHandleReq}
\end{requirement}
\caption{Requirements for receiving a request (used by the scheduler)}\label{recvReq}
\label{}
\end{figure}

\begin{figure}\small
\begin{requirement}
Address $a$ present in cache node $c$ can not be chosen for eviction by a thread
executing in $c$ if either
\begin{enumerate}
\item address $a$ has been chosen for eviction by some thread and that thread
has not yet replaced $a$, or
\item a thread in $c$ is handling a request for address
$a$ from any source
\end{enumerate}\label{evict}
\caption{Requirements for suspending a thread during choosing an address for eviction}
\end{requirement}
\end{figure}

Requirements \ref{pHandleReq}, \ref{cHandleReq} and \ref{evict} are needed
because once a thread has finished up-to a point in its execution, it can not
re-execute that portion. For instance, consider a thread handling an upgrade
request from a child. Let's say thread $t$ has progressed to the point where it
is simply waiting to send an upgrade response to the child. Now, if the line
gets evicted because it was chosen for replacement, or it got downgraded by the
parent, then thread $t$ can not re-execute its procedure again. To prevent this,
only one thread is allowed to operate on a particular address, except in the
case of downgrade requests from the parent. In this case, a thread is created to
handle the downgrade request from the parent even if there is another thread
handling a request for the same address from a child, if the latter thread is
just waiting for a response from the parent. This is done to avoid deadlocks
like the ones shown in Figure \ref{unknown}.

\begin{figure}\small
\begin{requirement}
Starvation-freedom: A thread that keeps becoming ready to execute will
eventually be scheduled to execute.\label{starvation}
\end{requirement}
\end{figure}

Local starvation-freedom is required in each cache node. If a cache node never
executes a particular suspended thread, even if it keeps getting ready to
execute, then the thread will not progress, leading to a deadlock.

A scheduler can be implemented as described in Figure \ref{scheduler}, obeying
Requirements \ref{pHandleReq} and \ref{cHandleReq}. It should perform some form of
fair arbiration among its suspended threads to preserve Requirement
\ref{starvation}.

\newcommand{\lWhile}{\textbf{while}}
\newcommand{\lIf}{\textbf{if}}
\newcommand{\lElsIf}{\textbf{else if}}
\newcommand{\lElse}{\textbf{else}}

\floatstyle{plain}
\restylefloat{figure}

\begin{figure}
\small
\begin{boxedminipage}{\linewidth}
\begin{alltt}
\normalfont
\lWhile{} (\(True\)) \bopen
      \lIf (\Resp{c}{n}{a}{x} is in the input channel,
            where \(n = c.parent\)) \bopen
            \lIf (suspended thread \(t\) is waiting for 
                 a response from \(c\) for address \(a\)) \bopen
                   \resume{} \(t\);
            \bclose \lElse \bopen
                   \receive{} \Resp{c}{n}{a}{x};
                   \start{} \dRespL(\(c, n, a, x\));
            \bclose
      \bclose \lElsIf (\Resp{p}{n}{a}{x} is in the input channel,
            where \(p = n.parent\)) \bopen
                   // There must be a suspended thread \(t\)
                   waiting for a response from \(p\) for address \(a\)
                   \resume{} \(t\);
            \bclose
      \bclose \lElsIf (\Req{c}{n}{a}{x} is in the input channel,
                     where \(n = c.parent\)) \bopen
             \lIf (Local-Property \ref{pHandleReq} is satisfied) \bopen
                   \receive{} \Req{c}{n}{a}{x};
                   \start{} \uReq(\(c, n, a, x\));
             \bclose
      \bclose \lElsIf (\Req{p}{n}{a}{x} is in the input channel,
                  where \(p = n.parent\)) \bopen
             \lIf (Local-Property \ref{cHandleReq} is satisfied) \bopen
                   \receive{} \Req{p}{n}{a}{x};
                   \start{} \dReq(\(p, n, a, x\));
             \bclose
      \bclose \lElsIf (suspended thread \(t\) is ready to execute) 
             \resume{} \(t\);
\bclose
\end{alltt}
\end{boxedminipage}
\caption{Implementation of a scheduler for cache node $n$}
\label{scheduler}
\end{figure}
